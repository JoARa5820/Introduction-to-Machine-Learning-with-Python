#-----------------------------------------------------------------------------#

### 2.3.3 선형 모델

#-----------------------------------------------------------------------------#

## 회귀의 선형 모델

# wave 데이터셋에 대한 선형 모델의 예측
mglearn.plots.plot_linear_regression_wave()

#-----------------------------------------------------------------------------#

## 선형 회귀(최소제곱법)

from sklearn.linear_model import LinearRegression

# wave 데이터셋 이용
x,y = mglearn.datasets.make_wave(n_samples=60)
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42)

lr = LinearRegression().fit(x_train,y_train)

print('lr.coef_:', lr.coef_) # 기울기 파라미터=가중치=계수
print('lr.intercept_:',lr.intercept_) # 편향=절편 파라미터

print('훈련 세트 점수: {:.2f}'.format(lr.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(lr.score(x_test,y_test)))


# boston 데이터셋 이용
x,y = mglearn.datasets.load_extended_boston()
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)
lr = LinearRegression().fit(x_train,y_train)

print('훈련 세트 점수: {:.2f}'.format(lr.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(lr.score(x_test,y_test)))

#-----------------------------------------------------------------------------#

## 리지(Ridge)회귀 (boston 데이터셋 이용)

from sklearn.linear_model import Ridge

# alpha 기본값은 1
ridge = Ridge().fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(ridge.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(ridge.score(x_test,y_test)))


ridge10 = Ridge(alpha=10).fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(ridge10.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(ridge10.score(x_test,y_test)))


ridge01 = Ridge(alpha=0.1).fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(ridge01.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(ridge01.score(x_test,y_test)))


# 선형 회귀와 몇 가지 alpha 값을 가진 리지 회귀의 계수 크기 비교
# 묶어서 돌리기
plt.plot(ridge10.coef_,'^',label='Ridge alpha=10') # 리지회귀 10
plt.plot(ridge.coef_,'s',label='Ridge alpha=1') # 리지회귀 1
plt.plot(ridge01.coef_,'v',label='Ridge alpha=0.1') # 리지회귀 0.1
plt.plot(lr.coef_,'o',label='LinearRegression') # 선형회귀
plt.xlabel('계수 목록')
plt.ylabel('계수 크기')
xlims = plt.xlim()
plt.hlines(0,xlims[0],xlims[1])
plt.xlim(xlims)
plt.ylim(-25,25)
plt.legend()


# 보스턴 주택가격 데이터셋에 대한 리지회귀와 선형회귀의 학습 곡선
mglearn.plots.plot_ridge_n_samples()

#-----------------------------------------------------------------------------#

## 라소
from sklearn.linear_model import Lasso

lasso = Lasso().fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(lasso.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(lasso.score(x_test,y_test)))
print('사용한 특성의 개수:',np.sum(lasso.coef_ != 0))

# 과소적합 줄이기 위해 alpha값 줄이기
# max_iter(반복 실행하는 최대 횟수)의 기본값 늘리기
lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(lasso001.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(lasso001.score(x_test,y_test)))
print('사용한 특성의 개수:',np.sum(lasso001.coef_ != 0))


lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(x_train,y_train)
print('훈련 세트 점수: {:.2f}'.format(lasso00001.score(x_train,y_train)))
print('테스트 세트 점수: {:.2f}'.format(lasso00001.score(x_test,y_test)))
print('사용한 특성의 개수:',np.sum(lasso00001.coef_ != 0))


# 리지회귀와 alpha 값이 다른 라소 회귀의 계수 크기 비교
# 묶어서 돌리기
plt.plot(lasso.coef_,'s',label='lasso alpha=1') # 라소 1
plt.plot(lasso001.coef_,'^',label='lasso alpha=0.01') # 라소 0.01
plt.plot(lasso00001.coef_,'v',label='lasso alpha=0.0001') # 라소 0.0001

plt.plot(ridge01.coef_,'o',label='Ridge alpha=0.1') # 리지회귀 0.1

plt.legend(ncol=2, loc=(0, 1.05))
plt.ylim(-25,25)
plt.xlabel('계수 목록')
plt.ylabel('계수 크기')

# 보통은 리지회귀를 선호 / 특성이 많고 그중 일부분만 중요하다면 Lasso가 더 나음

#-----------------------------------------------------------------------------#

## 분류용 선형 모델

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

# forge 데이터셋 이용
x,y = mglearn.datasets.make_forge()


# forge 데이터셋에 기본 매개변수를 사용해 만든 선형 SVM과 로지스틱 회귀 모델의 결정 경계
# 묶어서 돌리기
fig,axes = plt.subplots(1,2,figsize=(10,3))

for model,ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(x,y)
    mglearn.plots.plot_2d_separator(clf,x,fill=False,eps=0.5,ax=ax,alpha=.7)
    mglearn.discrete_scatter(x[:,0],x[:,1],y,ax=ax)
    ax.set_title(clf.__class__.__name__)
    ax.set_xlabel('특성 0')
    ax.set_ylabel('특성 1')
axes[0].legend()

# 규제의 강도를 결정하는건 매개변수 C / C가 커지면 규제 감소
# 각기 다른 C값으로 만든 선형 SVM 모델의 결정 경계
mglearn.plots.plot_linear_svc_regularization()



# 유방암 데이터셋 이용
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
x_train,x_test,y_train,y_test = train_test_split(
        cancer.data, cancer.target, stratify=cancer.target, random_state=42)
logreg = LogisticRegression().fit(x_train,y_train)
print('훈련 세트 점수: {:.3f}'.format(logreg.score(x_train,y_train)))
print('테스트 세트 점수: {:.3f}'.format(logreg.score(x_test,y_test)))


logreg100 = LogisticRegression(C=100).fit(x_train,y_train)
print('훈련 세트 점수: {:.3f}'.format(logreg100.score(x_train,y_train)))
print('테스트 세트 점수: {:.3f}'.format(logreg100.score(x_test,y_test)))


logreg001 = LogisticRegression(C=0.01).fit(x_train,y_train)
print('훈련 세트 점수: {:.3f}'.format(logreg001.score(x_train,y_train)))
print('테스트 세트 점수: {:.3f}'.format(logreg001.score(x_test,y_test)))


# 유방암 데이터셋에 각기 다른 C값을 사용해서 만든 로지스틱 회귀 계수
# 묶어서 돌리기
plt.plot(logreg100.coef_.T,'^',label='C=100')
plt.plot(logreg.coef_.T,'o',label='C=1')
plt.plot(logreg001.coef_.T,'v',label='C=0.001')
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0,xlims[0],xlims[1])
plt.xlim(xlims)
plt.ylim(-5,5)
plt.xlabel('특성')
plt.ylabel('계수 크기')
plt.legend()



# 유방암 데이터와 L1규제를 사용해 각기 다른 C값을 적용한 로지스틱 회귀 모델의 계수
for C,marker in zip([0.001,1,100],['o','^','v']):
    lr_l1 = LogisticRegression(C=C,penalty='l1').fit(x_train,y_train)
    print('C={:.3f} 인 l1 로지스틱 회귀의 훈련 정확도: {:.2f}'.format(
            C, lr_l1.score(x_train,y_train)))
    print('C={:.3f} 인 l1 로지스틱 회귀의 테스트 정확도: {:.2f}'.format(
            C, lr_l1.score(x_test,y_test)))
    plt.plot(lr_l1.coef_.T,marker,label='C={:.3f}'.format(C))

plt.xticks(range(cancer.data.shape[1]),cancer.feature_names,rotation=90)
xlims = plt.xlim()
plt.hlines(0,xlims[0],xlims[1])
plt.xlim(xlims)
plt.xlabel('특성')
plt.ylabel('계수 크기')

plt.ylim(-5,5)
plt.legend(loc=3)

#-----------------------------------------------------------------------------#

## 다중 클래스 분류용 선형 모델

# 세 개의 클래스를 가진 2차원 데이터셋
from sklearn.datasets import make_blobs

# 묶어서 돌리기
x,y = make_blobs(random_state=42)
mglearn.discrete_scatter(x[:,0],x[:,1],y)
plt.xlabel('특성 0')
plt.ylabel('특성 1')
plt.legend(['클래스 0','클래스 1','클래스 2'])


# LinearSVC 분류기 훈련
linear_svm = LinearSVC().fit(x,y)
print('계수 배열의 크기: ',linear_svm.coef_.shape)
print('절편 배열의 크기: ',linear_svm.intercept_.shape)


# 세 개의 이진 분류기가 만드는 경계 시각화 / 세 개의 일대다 분류기가 만든 결정 경계
mglearn.discrete_scatter(x[:,0],x[:,1],y)
line = np.linspace(-15,15)
for coef,intercept,color in zip(linear_svm.coef_,linear_svm.intercept_,mglearn.cm3.colors):
    plt.plot(line, -(line*coef[0]+intercept) / coef[1], c = color)
plt.ylim(-10,15)
plt.xlim(-10,8)
plt.xlabel('특성 0')
plt.ylabel('특성 1')
plt.legend(['클래스 0','클래스 1','클래스 2','클래스 0 경계','클래스 1 경계','클래스 2 경계'], loc=(1.01, 0.3))

#-----------------------------------------------------------------------------#
