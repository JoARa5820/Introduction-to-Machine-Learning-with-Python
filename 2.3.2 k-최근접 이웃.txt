#-----------------------------------------------------------------------------#

### 2.3.2 k-최근접 이웃

#-----------------------------------------------------------------------------#

## k-최근접 이웃 분류

# forge 데이터셋에 대한 1-최근접 이웃 모델의 예측
mglearn.plots.plot_knn_classification(n_neighbors=1)

# forge 데이터셋에 대한 3-최근접 이웃 모델의 예측
mglearn.plots.plot_knn_classification(n_neighbors=3)

# 일반화 성능 평가를 위해 훈련 세트와 테스트 세트로 나누기
from sklearn.model_selection import train_test_split
x,y = mglearn.datasets.make_forge()
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)


from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3)

# 훈련 세트로 분류 모델 학습시키기
clf.fit(x_train,y_train)

# 테스트 데이터에 대한 예측
print('테스트 세트 예측:', clf.predict(x_test))

# 모델의 일반화 평가
print('테스트 세트 정확도:{:.2f}'.format(clf.score(x_test,y_test)))

#-----------------------------------------------------------------------------#

## KNeighborsClassifier 분석

# n_neighbors 값이 각기 다른 최근접 이웃 모델이 만든 결정 경계
fig,axes = plt.subplots(1,3,figsize=(10,3)) 

for n_neighbors,ax in zip([1,3,9],axes):
    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(x,y)
    mglearn.plots.plot_2d_separator(clf,x,fill=True,eps=0.5,ax=ax,alpha=.4)
    mglearn.discrete_scatter(x[:,0],x[:,1],y,ax=ax)
    ax.set_title('{} 이웃'.format(n_neighbors))
    ax.set_xlabel('특성 0')
    ax.set_ylabel('특성 1')
axes[0].legend(loc=3)


from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
x_train,x_test,y_train,y_test = train_test_split(
        cancer.data,cancer.target,stratify=cancer.target,random_state=66)

training_accuracy = []
test_accuracy = []

neighbors_settings = range(1,11)


for n_neighbors in neighbors_settings:
    # 모델 생성
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(x_train,y_train)
    # 훈련 세트 정확도 저장
    training_accuracy.append(clf.score(x_train,y_train))
    # 일반화 정확도 저장
    test_accuracy.append(clf.score(x_test,y_test))


# n_neighbors 변화에 따른 훈련 정확도와 테스트 정확도
plt.plot(neighbors_settings, training_accuracy, label='훈련 정확도')
plt.plot(neighbors_settings, test_accuracy, label='테스트 정확도')
plt.ylabel('정확도')
plt.xlabel('n_neighbors')
plt.legend()
    
#-----------------------------------------------------------------------------#

## k-최근접 이웃 회귀

# wave 데이터셋에 대한 1-최근접 이웃 회귀 모델의 예측
mglearn.plots.plot_knn_regression(n_neighbors=1)

# wave 데이터셋에 대한 3-최근접 이웃 회귀 모델의 예측
mglearn.plots.plot_knn_regression(n_neighbors=3)


from sklearn.neighbors import KNeighborsRegressor
x,y = mglearn.datasets.make_wave(n_samples=40)
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)

reg = KNeighborsRegressor(n_neighbors=3)
reg.fit(x_train,y_train)

print('테스트 세트 예측:\n', reg.predict(x_test))

print('테스트 세트 R^2:{:.2f}'.format(reg.score(x_test,y_test)))

#-----------------------------------------------------------------------------#

## KNeighborsRegressor 분석

# 한번에 쭉 돌리기(n_neighbors 값에 따라 최근접 이웃 회귀로 만들어진 예측 비교)
fig,axes = plt.subplots(1,3,figsize=(15,4))
# -3과 3 사이에 1000개의 데이터포인트를 만듦
line = np.linspace(-3,3,1000).reshape(-1,1)

# 1,3,9 이웃을 사용한 예측
for n_neighbors,ax in zip([1,3,9],axes):
    reg = KNeighborsRegressor(n_neighbors=n_neighbors)
    reg.fit(x_train,y_train)
    ax.plot(line,reg.predict(line))
    ax.plot(x_train,y_train,'^',c=mglearn.cm2(0),markersize=8)
    ax.plot(x_test,y_test,'v',c=mglearn.cm2(1),markersize=8)
    
    ax.set_title(
            '{} 이웃의 훈련 스코어: {:.2f} 테스트 스코어: {:.2f}'.format(
                    n_neighbors,reg.score(x_train,y_train),
                    reg.score(x_test,y_test)))
    ax.set_xlabel('특성')
    ax.set_ylabel('타깃')
axes[0].legend(['모델 예측','훈련 데이터/타깃','테스트 데이터/타깃'], loc = 'best')

#-----------------------------------------------------------------------------#
